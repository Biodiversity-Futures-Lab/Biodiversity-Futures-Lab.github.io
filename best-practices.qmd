---
title: "Best Practices"
---

Welcome to the *Biodiversity Futures lab* software development best practices guide! This is a short document outlining the best practices for developing software within the BFL, designed to get programmers up and running quickly, to make contributions.

# Code style

Use [Google style guide for R](https://google.github.io/styleguide/Rguide.html), which is itself a fork of the [tidyverse style guide](https://style.tidyverse.org/). This adds some sensible additions on top of the tidyverse style guide, such as using explicit returns from functions, and qualifying namespaces (e.g., always using the double colon syntax `purrr::map()`).

If you want to use `{lintr}` to check adherence to the style guide, then check the package repos for the appropriate `.lintr` file.

If writing C++ extensions (via `Rcpp`) then we use the [Google style guide](https://google.github.io/styleguide/cppguide.html).

# Unit testing

For unit testing we use `{testthat}`. Follow the guides on the [website](https://testthat.r-lib.org/index.html) to get set up in your package, with the use of [usethis](https://github.com/r-lib/usethis). If youâ€™ve got `{usethis}` installed, then from the repo you want to test, you run `usethis:use_test("name")`.

You can use testing outside of package development, for example, within the structure of a `{targets}` repo, through direct imports of your `R` code through the use of `source(...)`.

# Version control

For version control, we use Git, with the repository hosted on GitHub. Depending on the circumstances, host the repo either on your own personal repo, or, on the BFL organisation repo. Set it to either private or public depending on the circumstances of the work.

For branching we use the [GitHub-flow](https://githubflow.github.io/) strategy. This is a simple branching model which is perfect for our small team (and as far as I can tell, is the standard for R package development).

In essence, whenever you want to work on something new, create a new branch off `main` with a descriptive name, then when you are finished, create a *pull request* to merge it back into `main`. This allows us to work continuously in a fashion similar to many other R packages, without unnecessary overhead with separate `develop` and `main` branches.

In more detail, this process is:

1. When you want to work on something new, create a branch off `main`. This will have a descriptive name, such as `issue-32`, or `fix-projection-bug`, etc. Work on this branch until all your stuff is implemented. Be sure to constantly push to your development branch.
2. When ready to be merged back into `main`, make a *pull request* and it will be merged back into `main`. When developing, make sure all the tests pass and (if applicable) any BII pipelines finish running to completion at 10km resolution.
3. Repeat this process until ready for a release, adding features/fixes as we go.
4. When ready for a release, we can simply tag the appropriate version number and create a release *on GitHub*.

**NB:** For branching we previously tried the [Git-flow](https://nvie.com/posts/a-successful-git-branching-model/) branching strategy. However after trying this for 6 months, it added unnecessary complications to our workflow, as we typically don't need to maintain older versions of the pipeline; we implement new fixes in a rolling-release, like most other R packages.

## Issues

If something looks wrong, then make an issue. This will then be discussed and the team will prioritise and work on it as required. Once the fix has been implemented and merged into the `main` branch, then the issue will be closed.

# Documentation

All functions that are written and committed should be documented. Follow the `tidyverse` style guide when writing this documentation. So the below for a quick-reference example:

``` r
#' Run a specific target and run the pipeline
#' @details Run a single target, after invalidating the target. Rebuilds the
#'   "_targets.Rmd" to re-generate all the necessary running stuff.
#'
#' @param target A string giving the target to be profiled.
#' @param file_out A string giving the location to save the profile output.
RunTarget <- function(target, pipe_file = "_targets.Rmd") {
  rmarkdown::render(pipe_file)

  message("Warning! We are invalidating the target now!")
  targets::tar_invalidate(target)
  targets::tar_invalidate(target)

  targets::tar_make(target, callr_function = NULL, use_crew = FALSE)
}
```

For full details see [here](https://style.tidyverse.org/documentation.html).

# Code reviews

When doing code reviews we follow the [Google code review guidelines](https://google.github.io/eng-practices/review/). Borrowed from this guide (and the NHM Dev wiki), in short, we suggest the following things be asked during a code review:

- *Design*: Is the code well-designed and appropriate for your system?
- *Functionality*: Does the code behave as the author likely intended? Is the way the code behaves good for its users?
- *Complexity*: Could the code be made simpler? Would another developer be able to easily understand and use this code when they come across it in the future?
- *Tests*: Does the code have correct and well-designed automated tests?
- *Naming*: Did the developer choose clear names for variables, classes, methods, etc.?
- *Comments*: Are the comments clear and useful?
- *Style*: Does the code follow our style guides?
- *Documentation*: Did the developer also update relevant documentation?

You can request a code review on any code you would like to be reviewed, be it a script, a package, or an `Rmarkdown` file.

## Reviewing pull requests

When reviewing a pull request the following guidelines are helpful. Unless the changes are very minor, try to pull the code and at least run the tests on your own machine. When looking at the changes, use the following as a general guideline:

- Is it clear what the feature/fix the PR is addressing?
- Does the code pass tests when run locally?
- Is the style guide followed? Is the code clean and well-documented?
- If there are new package dependencies: are they popular/well-maintained?
- If there are any architectural changes: are they sound choices?
- If it is a scientific contribution: is the methodology sensible?

Mechanically here is a guide to what to do when reviewing a pull request (using Git on the command line, from the base directory of the repository):

1. Checkout the branch that is going to be merged: `git checkout branch_to_be_merged`.
2. Pull the changes from the origin repository `git pull origin`. You should see some file deltas here.
3. (If using `{renv}`) Update the environment with `renv::restore()` from within R.
4. Run the tests: if within a package just run `devtools::test()`, or if not run `testthat::test_dir()`.

Having done these steps we have a minimal procedure to check that the code runs and passes the tests on another machine. From here we can check the above guidelines to be sure that the changes are sensible.

# Rstudio (IDEs)

You can use any IDE you want to work with. Generally, however, we have the most experience with using just Rstudio or Rstudio with an older-school editor such as Vim or Emacs.

# Miscellanea

- In a perfect world seperate functions for analysis from the analysis piepline itself. **Turn analysis functions into a package, stored separate from the actual data analysis pipeline** Then you can unit test them, share them with other more easily, and seperate out *package development* from *data analysis*.
- Use `{targets}` when writing data analysis pipelines to keep track of dependencies and avoid wasting compute resources.
- Use `{logger}` or `{logr}` for logging. When adding logging to a project, a good strategy is to just replace comments with logging calls.
- We use `{assertthat}` for assertions to program defensively. This is very useful in functions which may or may not be tested, to sanity check that you are doing what you want to be doing.
- **TODO:** Continuous integration (CI)
