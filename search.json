[
  {
    "objectID": "wiki.html",
    "href": "wiki.html",
    "title": "BFL Wiki",
    "section": "",
    "text": "This is a landing page for various resources related to day-to-day operations in the lab. Chiefly:"
  },
  {
    "objectID": "wiki.html#welcome-to-the-lab",
    "href": "wiki.html#welcome-to-the-lab",
    "title": "BFL Wiki",
    "section": "Welcome to the Lab!",
    "text": "Welcome to the Lab!"
  },
  {
    "objectID": "wiki.html#best-practices",
    "href": "wiki.html#best-practices",
    "title": "BFL Wiki",
    "section": "Best practices",
    "text": "Best practices\nWelcome to the Biodiversity Futures lab software development best practices guide! This is a short document outlining the best practices for developing software within the BFL, designed to get programmers up and running quickly, to make contributions.\n\nCode style\nUse Google style guide for R, which is itself a fork of the tidyverse style guide. This adds some sensible additions on top of the tidyverse style guide, such as using explicit returns from functions, and qualifying namespaces (e.g., always using the double colon syntax purrr::map()).\nIf you want to use {lintr} to check adherence to the style guide, then check the package repos for the appropriate .lintr file.\nIf writing C++ extensions (via Rcpp) then we use the Google style guide.\n\n\nUnit testing\nFor unit testing we use {testthat}. Follow the guides on the website to get set up in your package, with the use of usethis. If you’ve got {usethis} installed, then from the repo you want to test, you run usethis:use_test(\"name\").\nYou can use testing outside of package development, for example, within the structure of a {targets} repo, through direct imports of your R code through the use of source(...).\n\n\nVersion control\nFor version control, we use Git, with the repository hosted on GitHub. Depending on the circumstances, host the repo either on your own personal repo, or, on the BFL organisation repo. Set it to either private or public depending on the circumstances of the work.\nFor branching we use the GitHub-flow strategy. This is a simple branching model which is perfect for our small team (and as far as I can tell, is the standard for R package development).\nIn essence, whenever you want to work on something new, create a new branch off main with a descriptive name, then when you are finished, create a pull request to merge it back into main. This allows us to work continuously in a fashion similar to many other R packages, without unnecessary overhead with separate develop and main branches.\nIn more detail, this process is:\n\nWhen you want to work on something new, create a branch off main. This will have a descriptive name, such as issue-32, or fix-projection-bug, etc. Work on this branch until all your stuff is implemented. Be sure to constantly push to your development branch.\nWhen ready to be merged back into main, make a pull request and it will be merged back into main. When developing, make sure all the tests pass and (if applicable) any BII pipelines finish running to completion at 10km resolution.\nRepeat this process until ready for a release, adding features/fixes as we go.\nWhen ready for a release, we can simply tag the appropriate version number and create a release on GitHub.\n\nNB: For branching we previously tried the Git-flow branching strategy. However after trying this for 6 months, it added unnecessary complications to our workflow, as we typically don’t need to maintain older versions of the pipeline; we implement new fixes in a rolling-release, like most other R packages.\n\n\nIssues\nIf something looks wrong, then make an issue. This will then be discussed and the team will prioritise and work on it as required. Once the fix has been implemented and merged into the main branch, then the issue will be closed.\n\n\nDocumentation\nAll functions that are written and committed should be documented. Follow the tidyverse style guide when writing this documentation. So the below for a quick-reference example:\n#' Run a specific target and run the pipeline\n#' @details Run a single target, after invalidating the target. Rebuilds the\n#'   \"_targets.Rmd\" to re-generate all the necessary running stuff.\n#'\n#' @param target A string giving the target to be profiled.\n#' @param file_out A string giving the location to save the profile output.\nRunTarget &lt;- function(target, pipe_file = \"_targets.Rmd\") {\n  rmarkdown::render(pipe_file)\n\n  message(\"Warning! We are invalidating the target now!\")\n  targets::tar_invalidate(target)\n  targets::tar_invalidate(target)\n\n  targets::tar_make(target, callr_function = NULL, use_crew = FALSE)\n}\nFor full details see here.\n\n\nCode reviews\nWhen doing code reviews we follow the Google code review guidelines. Borrowed from this guide (and the NHM Dev wiki), in short, we suggest the following things be asked during a code review:\n\nDesign: Is the code well-designed and appropriate for your system?\nFunctionality: Does the code behave as the author likely intended? Is the way the code behaves good for its users?\nComplexity: Could the code be made simpler? Would another developer be able to easily understand and use this code when they come across it in the future?\nTests: Does the code have correct and well-designed automated tests?\nNaming: Did the developer choose clear names for variables, classes, methods, etc.?\nComments: Are the comments clear and useful?\nStyle: Does the code follow our style guides?\nDocumentation: Did the developer also update relevant documentation?\n\nYou can request a code review on any code you would like to be reviewed, be it a script, a package, or an Rmarkdown file.\n\nReviewing pull requests\nWhen reviewing a pull request the following guidelines are helpful. Unless the changes are very minor, try to pull the code and at least run the tests on your own machine. When looking at the changes, use the following as a general guideline:\n\nIs it clear what the feature/fix the PR is addressing?\nDoes the code pass tests when run locally?\nIs the style guide followed? Is the code clean and well-documented?\nIf there are new package dependencies: are they popular/well-maintained?\nIf there are any architectural changes: are they sound choices?\nIf it is a scientific contribution: is the methodology sensible?\n\nMechanically here is a guide to what to do when reviewing a pull request (using Git on the command line, from the base directory of the repository):\n\nCheckout the branch that is going to be merged: git checkout branch_to_be_merged.\nPull the changes from the origin repository git pull origin. You should see some file deltas here.\n(If using {renv}) Update the environment with renv::restore() from within R.\nRun the tests: if within a package just run devtools::test(), or if not run testthat::test_dir().\n\nHaving done these steps we have a minimal procedure to check that the code runs and passes the tests on another machine. From here we can check the above guidelines to be sure that the changes are sensible.\n\n\n\nRstudio (IDEs)\nYou can use any IDE you want to work with. Generally, however, we have the most experience with using just Rstudio or Rstudio with an older-school editor such as Vim or Emacs.\n\n\nMiscellanea\n\nIn a perfect world seperate functions for analysis from the analysis piepline itself. Turn analysis functions into a package, stored separate from the actual data analysis pipeline Then you can unit test them, share them with other more easily, and seperate out package development from data analysis.\nUse {targets} when writing data analysis pipelines to keep track of dependencies and avoid wasting compute resources.\nUse {logger} or {logr} for logging. When adding logging to a project, a good strategy is to just replace comments with logging calls.\nWe use {assertthat} for assertions to program defensively. This is very useful in functions which may or may not be tested, to sanity check that you are doing what you want to be doing.\nTODO: Continuous integration (CI)"
  },
  {
    "objectID": "outputs.html#talks",
    "href": "outputs.html#talks",
    "title": "Outputs",
    "section": "Talks",
    "text": "Talks"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Biodiversity Futures Lab is based at the Natural History Museum in London."
  },
  {
    "objectID": "about.html#our-projects",
    "href": "about.html#our-projects",
    "title": "About",
    "section": "Our Projects",
    "text": "Our Projects\nThe Biodiversity Futures Lab at the Natural History Museum in London explores how environmental change affects biodiversity using cutting-edge research. Projects include studying the effects of ocean acidification on marine bryozoans, predicting climate-driven changes in body size, and using AI to analyze plant traits for biodiversity indicators. Additionally, the lab employs genomic tools to accelerate the discovery of parasitoid wasps, enhancing their potential as biocontrol agents. By integrating advanced technologies, the lab aims to improve our understanding of biodiversity and its future."
  },
  {
    "objectID": "about.html#our-team",
    "href": "about.html#our-team",
    "title": "About",
    "section": "Our Team",
    "text": "Our Team\nThe following very clever people work in the BFL:\n\nProfessor Andy Purvis: Research Leader\nDr Adriana De Palma: Principal Researcher\nSara Contu: Biodiversity Data Supervisor\nIzzi Strudwick: Project Coordinator\nDr Connor Duffin: Research Software Engineer\nDr Alexa Varah: Postdoctoral Researcher\nDr Patrick Walkden: Postdoctoral Researcher\nJustin Isip: PhD student\nSophie Jane Tudge: PhD student"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Biodiversity Futures Lab",
    "section": "",
    "text": "Welcome to the Biodiversity Futures Lab website! This is a work-in-progress and will be updated as we add content to it - bear with us!\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "News",
    "section": "",
    "text": "New paper in Science (4th April 2025)\nAre insects in rapid global decline? If so, what should be done to turn things around? The latest paper from the GLiTRS project, Integrating multiple evidence streams to understand insect biodiversity change, spells out why these are still open questions, despite their high profile over recent years. It argues that time-series data, such as those coming from monitoring programmes, are too patchy and biased to give a clear picture of any global trend, and that combining different lines of evidence into a synthetic threat-response model can provide a deeper understanding. Andy was interviewed for an article about the paper on the NHM news site."
  }
]